{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fix_sys_path\n",
    "from plasma_classes import *\n",
    "from plasma_utils import *\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from tempfile import TemporaryDirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_over_states(file_path: str):\n",
    "    \"\"\"\n",
    "    A generator that yields the system's state at each iteration from a binary file.\n",
    "\n",
    "    file_path: str - the path to the binary file to read from.\n",
    "    Yields:\n",
    "        - A tuple containing the nodes object, the particles object, and the walls object.\n",
    "    \"\"\"\n",
    "    # Open the file in read-only mode.\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        # Compute the position in the file where the serialized data begins.\n",
    "        header_size = 4  # The size of the header containing the size of the serialized data.\n",
    "        data_offset = 0\n",
    "\n",
    "        while True:\n",
    "            # Move the file pointer to the beginning of the serialized data.\n",
    "            f.seek(data_offset, os.SEEK_SET)\n",
    "\n",
    "            # Read the size of the serialized data.\n",
    "            size_bytes = f.read(header_size)\n",
    "            if not size_bytes:\n",
    "                # No more data in the file.\n",
    "                break\n",
    "            size = int.from_bytes(size_bytes, byteorder=\"big\")\n",
    "\n",
    "            # Compute the position in the file where the next serialized data begins.\n",
    "            data_offset = f.tell() + size\n",
    "\n",
    "            # Read the serialized data.\n",
    "            serialized_bytes = f.read(size)\n",
    "            serialized_data = pickle.loads(serialized_bytes)\n",
    "\n",
    "            # Yield the iteration number, nodes, particles, and walls from the deserialized data.\n",
    "            yield serialized_data[\"nodes\"], serialized_data[\"particles\"], serialized_data[\"walls\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def deepcopy_nodes(nodes):\n",
    "    new_nodes = Nodes(nodes.length - 1)\n",
    "    new_nodes.rho = np.copy(nodes.rho)\n",
    "    new_nodes.E = np.copy(nodes.E)\n",
    "    new_nodes.phi = np.copy(nodes.phi)\n",
    "    new_nodes.electricEnergy = np.copy(nodes.electricEnergy)\n",
    "    new_nodes.conc_e = np.copy(nodes.conc_e)\n",
    "    new_nodes.conc_i = np.copy(nodes.conc_i)\n",
    "    return new_nodes\n",
    "\n",
    "def deepcopy_wall(wall):\n",
    "    new_wall = Wall(wall.left, wall.right, wall.number, wall.h, wall.side)\n",
    "    new_wall.particles_lst = copy.deepcopy(wall.particles_lst)\n",
    "    return new_wall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "-1\n",
      "----\n",
      "-1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'particles_lst'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(nodes_0\u001b[39m.\u001b[39mrho, system_states[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mrho)\n\u001b[0;32m     34\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(particles_0\u001b[39m.\u001b[39mv, system_states[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mv)\n\u001b[1;32m---> 35\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(walls_0\u001b[39m.\u001b[39;49mparticles_lst, system_states[\u001b[39m0\u001b[39m][\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mparticles_lst)\n\u001b[0;32m     37\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(nodes_2\u001b[39m.\u001b[39mrho, system_states[\u001b[39m2\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mrho)\n\u001b[0;32m     38\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(particles_2\u001b[39m.\u001b[39mv, system_states[\u001b[39m2\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mv)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'particles_lst'"
     ]
    }
   ],
   "source": [
    "# Create a temporary directory to store the test files.\n",
    "# Set up the test data.\n",
    "n_macro = 10\n",
    "concentration = 1e15\n",
    "q = 1.602176634e-19\n",
    "m = 9.10938356e-31\n",
    "nodes = Nodes(10)\n",
    "particles = Particles(n_macro, concentration, -q, m)\n",
    "walls = [Wall(0, 0.1, 1, 1e-3, \"left\"), Wall(0.9, 1, 2, 1e-3, \"right\")]\n",
    "\n",
    "# Create a list to store the system's state for each iteration.\n",
    "system_states = []\n",
    "\n",
    "# Modify the system's state for 5 iterations and save the system's state after each iteration.\n",
    "for i in range(5):\n",
    "    # Modify the system's state.\n",
    "    nodes.rho += 10\n",
    "    particles.v += 1\n",
    "    walls[0].particles_lst = [particles]\n",
    "\n",
    "    # Save the system's state to a file.\n",
    "    file_path = \"test.bin\"\n",
    "    save_system_state(i, nodes, particles, walls, file_path)\n",
    "\n",
    "    # Append the system's state to the list.\n",
    "    system_states.append((deepcopy_nodes(nodes), particles.deepcopy(), deepcopy_wall(walls[0])))\n",
    "\n",
    "# Load the system's state for the first and third iterations.\n",
    "nodes_0, particles_0, walls_0 = load_system_state(\"test.bin\", 0)\n",
    "nodes_2, particles_2, walls_2 = load_system_state(\"test.bin\", 2)\n",
    "\n",
    "# Check that the loaded data matches the expected values.\n",
    "assert np.array_equal(nodes_0.rho, system_states[0][0].rho)\n",
    "assert np.array_equal(particles_0.v, system_states[0][1].v)\n",
    "assert np.array_equal(walls_0.particles_lst, system_states[0][2].particles_lst)\n",
    "\n",
    "assert np.array_equal(nodes_2.rho, system_states[2][0].rho)\n",
    "assert np.array_equal(particles_2.v, system_states[2][1].v)\n",
    "assert np.array_equal(walls_2.particles_lst, system_states[2][2].particles_lst)\n",
    "\n",
    "\n",
    "# Load the system's state for a non-existent iteration and check that an exception is raised.\n",
    "try:\n",
    "    load_system_state(os.path.join(\"test.bin\"), 5)\n",
    "    assert False, \"Expected ValueError not raised.\"\n",
    "except ValueError as e:\n",
    "    assert str(e) == \"No data for iteration 5 found in file test_0.bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walls_0[0].particles_lst[0].v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particles_0.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_states[0][0].rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_0.rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = Particles(10, 1.0, -1.0, 1.0)\n",
    "p1.x = np.arange(10)\n",
    "p1.v = np.arange(10)\n",
    "p2 = Particles(10, 1.0, -1.0, 1.0)\n",
    "p2.x = np.arange(10)\n",
    "p2.v = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar(first, second):\n",
    "    first.add(second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar(p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particles.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(particles):\n",
    "    mask = particles.x < 4\n",
    "    particles.delete(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo(particles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particles.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = particles.x < 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True, False, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles = particles[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particles.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time:  0.5579440593719482  seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Create 10 million particles\n",
    "particles = Particles(10000000, 1.0, -1.0, 1.0)\n",
    "\n",
    "# Modify individual particle velocities one at a time\n",
    "start_time = time.time()\n",
    "for i in range(5000000):\n",
    "    particles.v[i] += 0.1\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Elapsed time: \", end_time - start_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice1 = particles[3:5000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time:  0.0  seconds\n"
     ]
    }
   ],
   "source": [
    "# Modify individual particle velocities one at a time\n",
    "start_time = time.time()\n",
    "for i in range(3000):\n",
    "    slice1.v[i] += 1000\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Elapsed time: \", end_time - start_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean index operation took 0.0009968280792236328 seconds\n",
      "By-hand creation of new array with the same mask took 0.0009965896606445312 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Set up the particles array\n",
    "n_particles = 100000\n",
    "x = np.random.uniform(low=-1, high=1, size=n_particles)\n",
    "v = np.random.normal(loc=0, scale=1, size=n_particles)\n",
    "particles = Particles(n_particles, 1.0, 1.0, 1.0)\n",
    "particles.x = x\n",
    "particles.v = v\n",
    "\n",
    "# Create a boolean mask\n",
    "mask = np.random.choice([True, False], size=n_particles)\n",
    "\n",
    "# Test the boolean index operation\n",
    "t1 = time.time()\n",
    "sub_particles = particles[mask]\n",
    "t2 = time.time()\n",
    "print(f\"Boolean index operation took {t2-t1} seconds\")\n",
    "\n",
    "# Test the by-hand creation of new array with the same mask\n",
    "t3 = time.time()\n",
    "sub_particles = Particles(np.count_nonzero(mask), particles.concentration, particles.q, particles.m)\n",
    "sub_particles.x = particles.x[mask]\n",
    "sub_particles.v = particles.v[mask]\n",
    "t4 = time.time()\n",
    "print(f\"By-hand creation of new array with the same mask took {t4-t3} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create original particles object\n",
    "original_particles = Particles(10, 1.0, 1.0, 1.0)\n",
    "original_particles.x = np.random.rand(10)\n",
    "\n",
    "# make a deep copy of the original particles object\n",
    "copy_particles = original_particles.deepcopy()\n",
    "\n",
    "# modify the copy\n",
    "copy_particles.x[0] = 0.0\n",
    "\n",
    "# check that the original particles object is not modified\n",
    "assert np.allclose(original_particles.x[0], 0.0) == False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92823824, 0.76024596, 0.53362671, 0.0998817 , 0.07057969,\n",
       "       0.72800722, 0.4734317 , 0.77427962, 0.16064182, 0.1847532 ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_particles.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5336.26709348,  998.8169768 ,  705.79688562, 7280.07222921,\n",
       "       4734.31700472])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl = original_particles[2:7].deepcopy()\n",
    "sl.x *= 10000\n",
    "sl.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92823824, 0.76024596, 0.53362671, 0.0998817 , 0.07057969,\n",
       "       0.72800722, 0.4734317 , 0.77427962, 0.16064182, 0.1847532 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_particles.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2000.1, 2000.1, 2000.1, ...,    0. ,    0. ,    0. ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particles.v[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice6 = particles[3456 :5000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time:  0.6053729057312012  seconds\n"
     ]
    }
   ],
   "source": [
    "# Modify individual particle velocities one at a time\n",
    "start_time = time.time()\n",
    "for i in range(5000000):\n",
    "    slice1.v[i] -= 2000\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Elapsed time: \", end_time - start_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-999.9, -999.9, -999.9, ...,    0. ,    0. ,    0. ])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particles.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "slc = particles[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slc.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (0,) into shape (20,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m particles\u001b[39m.\u001b[39;49mx[:\u001b[39m20\u001b[39;49m] \u001b[39m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m slc\u001b[39m.\u001b[39mx\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (0,) into shape (20,)"
     ]
    }
   ],
   "source": [
    "particles.x[:20] = []\n",
    "slc.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Particles object with 100 particles\n",
    "particles = Particles(100, 1.0, -1.0, 1.0)\n",
    "\n",
    "# Select a slice of the particles\n",
    "sliced_particles = particles[10:30]\n",
    "\n",
    "# Double the speed of the sliced particles\n",
    "sliced_particles.v *= 2.0\n",
    "\n",
    "\n",
    "# Verify that the parent Particles object was updated\n",
    "assert np.allclose(particles.v[10:30], sliced_particles.v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Create instances of each class for testing\n",
    "particles = Particles(100, 1.0, 1.0, 1.0)\n",
    "particles.x = np.random.rand(100)\n",
    "particles.v = np.random.rand(100)\n",
    "nodes = Nodes(10)\n",
    "nodes.rho = np.random.rand(11)\n",
    "nodes.E = np.random.rand(11)\n",
    "nodes.phi = np.random.rand(11)\n",
    "wall = Wall(0, 1, 10, 0.1, 'left')\n",
    "particle1 = Particles(10, 0.5, 1.5, 1.0)\n",
    "particle1.x = np.random.rand(10)\n",
    "particle1.v = np.random.rand(10)\n",
    "particle2 = Particles(5, 0.5, 1.5, 1.0)\n",
    "particle2.x = np.random.rand(5)\n",
    "particle2.v = np.random.rand(5)\n",
    "wall.particles_lst = [particle1, particle2]\n",
    "\n",
    "# Test saving and loading of Particles objects\n",
    "particles_path = 'particles.pkl'\n",
    "save_to_file(particles, particles_path)\n",
    "loaded_particles = load_from_file(particles_path)\n",
    "assert isinstance(loaded_particles, Particles)\n",
    "assert loaded_particles.n_macro == particles.n_macro\n",
    "assert loaded_particles.concentration == particles.concentration\n",
    "assert np.allclose(loaded_particles.x, particles.x)\n",
    "assert np.allclose(loaded_particles.v, particles.v)\n",
    "\n",
    "# Test saving and loading of Nodes objects\n",
    "nodes_path = 'nodes.pkl'\n",
    "save_to_file(nodes, nodes_path)\n",
    "loaded_nodes = load_from_file(nodes_path)\n",
    "assert isinstance(loaded_nodes, Nodes)\n",
    "assert loaded_nodes.length == nodes.length\n",
    "assert np.allclose(loaded_nodes.rho, nodes.rho)\n",
    "assert np.allclose(loaded_nodes.E, nodes.E)\n",
    "assert np.allclose(loaded_nodes.phi, nodes.phi)\n",
    "\n",
    "# Test saving and loading of Wall objects\n",
    "wall_path = 'wall.pkl'\n",
    "save_to_file(wall, wall_path)\n",
    "loaded_wall = load_from_file(wall_path)\n",
    "assert isinstance(loaded_wall, Wall)\n",
    "assert loaded_wall.left == wall.left\n",
    "assert loaded_wall.right == wall.right\n",
    "assert loaded_wall.number == wall.number\n",
    "assert loaded_wall.h == wall.h\n",
    "assert loaded_wall.side == wall.side\n",
    "assert len(loaded_wall.particles_lst) == len(wall.particles_lst)\n",
    "assert isinstance(loaded_wall.particles_lst[0], Particles)\n",
    "assert loaded_wall.particles_lst[0].n_macro == particle1.n_macro\n",
    "assert loaded_wall.particles_lst[0].concentration == particle1.concentration\n",
    "assert loaded_wall.particles_lst[0].q == particle1.q\n",
    "assert loaded_wall.particles_lst[0].m == particle1.m\n",
    "assert np.allclose(loaded_wall.particles_lst[0].x, particle1.x)\n",
    "assert np.allclose(loaded_wall.particles_lst[0].v, particle1.v)\n",
    "assert isinstance(loaded_wall.particles_lst[1], Particles)\n",
    "assert loaded_wall.particles_lst[1].n_macro == particle2.n_macro\n",
    "assert loaded_wall.particles_lst[1].concentration == particle2.concentration\n",
    "assert loaded_wall.particles_lst[1].q == particle2.q\n",
    "assert loaded_wall.particles_lst[1].m == particle2.m\n",
    "assert np.allclose(loaded_wall.particles_lst[1].x, particle2.x)\n",
    "assert np.allclose(loaded_wall.particles_lst[1].v, particle2.v)\n",
    "\n",
    "# Clean up the test files\n",
    "os.remove(particles_path)\n",
    "os.remove(nodes_path)\n",
    "os.remove(wall_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
